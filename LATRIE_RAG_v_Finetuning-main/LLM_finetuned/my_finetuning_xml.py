# -*- coding: utf-8 -*-
"""my_finetuning_XML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j2Il2dEuxanK4GDI5_FqaV1wUFZaVXYC
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from peft import prepare_model_for_kbit_training
from peft import LoraConfig, get_peft_model
from datasets import load_dataset
import transformers
from sklearn.model_selection import train_test_split

# Load model

model_name = "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"
model = AutoModelForCausalLM.from_pretrained(model_name,
                                             device_map="auto", # automatically figures out how to best use CPU + GPU for loading model
                                             trust_remote_code=False, # prevents running custom model files on your machine
                                             revision="main") # which version of model to use in repo

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

# Using Base Model

model.eval() # model in evaluation mode (dropout modules are deactivated)

# craft prompt
question = "What is MDR-GNB?"
prompt=f'''[INST] {question} [/INST]'''

# tokenize input
inputs = tokenizer(prompt, return_tensors="pt")

# generate output
outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=140)

print(tokenizer.batch_decode(outputs)[0])

# Prompt Engineering

intstructions_string = f"""MedGPT, functioning as a virtual data science consultant on clinical questions, communicates in clear, accessible language, escalating to technical depth upon request. \
It reacts to questions aptly and ends responses with its signature '–MedGPT'. \
MedGPT will tailor the length of its responses to match the client's question, providing concise acknowledgments to brief expressions of gratitude or feedback, \
thus keeping the interaction natural and engaging.

Please respond to the following question.
"""
prompt_template = lambda question: f'''[INST] {intstructions_string} \n{question} \n[/INST]'''

prompt = prompt_template(question)
print(prompt)

# tokenize input
inputs = tokenizer(prompt, return_tensors="pt")

# generate output
outputs = model.generate(input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=140)

print(tokenizer.batch_decode(outputs)[0])

# Prepare Model for Training

model.train() # model in training mode (dropout modules are activated)

# enable gradient check pointing
model.gradient_checkpointing_enable()

# enable quantized training
model = prepare_model_for_kbit_training(model)

# LoRA config
config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

# LoRA trainable version of model
model = get_peft_model(model, config)

# trainable parameter count
model.print_trainable_parameters()

# Preparing Training Dataset

import os
import xml.etree.ElementTree as ET

class XMLParentMap:
    def __init__(self, tree):
        self.tree = tree
        self.parent_map = self.build_parent_map()

    def build_parent_map(self):
        parent_map = {}
        for p in self.tree.iter():
            for c in p:
                if c in parent_map:
                    parent_map[c].append(p)
                else:
                    parent_map[c] = [p]
        return parent_map

    def get_all_parents(self, element):
        all_parents = []
        self._collect_all_parents(element, all_parents)
        return all_parents

    def _collect_all_parents(self, element, all_parents):
        if element in self.parent_map:
            parents = self.parent_map[element]
            for parent in parents:
                all_parents.append(parent)
                self._collect_all_parents(parent, all_parents)

class XMLDirectoryReader:
    def __init__(self, directory):
        self.directory = directory

    def load_data(self):
        documents = []
        for filename in os.listdir(self.directory):
            if filename.endswith(".xml"):
                file_path = os.path.join(self.directory, filename)
                with open(file_path, 'r', encoding='utf-8') as file:
                    tree = ET.parse(file)
                    root = tree.getroot()
                    parent_map_instance = XMLParentMap(tree)
                    text_content = self.extract_text_from_xml(root, parent_map_instance)
                    if text_content:  # Vérifie que le texte n'est pas vide
                        documents.append(text_content)
        return documents

    def extract_text_from_xml(self, root, parent_map):
        namespaces = {'tei': 'http://www.tei-c.org/ns/1.0'}
        texts = []
        for elem in root.iter():
            all_parents = parent_map.get_all_parents(elem)

            # Vérifier si l'élément ou un de ses parents est dans <biblStruct>
            if any(p.tag in ['{http://www.tei-c.org/ns/1.0}biblStruct', '{http://www.tei-c.org/ns/1.0}figure'] for p in all_parents):
                continue  # Passer à l'élément suivant si dans <biblStruct>


            # Exclure certains éléments spécifiques (comme les métadonnées ou les informations de publication)
            if elem.tag in ['{http://www.tei-c.org/ns/1.0}title', '{http://www.tei-c.org/ns/1.0}head', '{http://www.tei-c.org/ns/1.0}p']:
                if elem.text and elem.text.strip():  # Exclure les éléments avec du texte vide
                    text = elem.text.strip()
                    if elem.tag == '{http://www.tei-c.org/ns/1.0}title':
                        text = f"\n\ntitle : {text}"  # Ajouter le préfixe "title :"
                    elif elem.tag == '{http://www.tei-c.org/ns/1.0}head':
                        text = f"\nhead : {text}"  # Ajouter le préfixe "head :"
                    texts.append(text)
        return "\n".join(texts)

data = XMLDirectoryReader("../data/Medical_Guidelines/XML").load_data()

print(len(data))

# Fonction pour segmenter le texte en utilisant les titres et les sous-titres
def segment_text(data):
    segments = []
    current_segment = []

    for text in data:
        lines = text.split("\n")
        for line in lines:
            if line.startswith("title :") or line.startswith("head :"):
                if current_segment:
                    segments.append("\n".join(current_segment).strip())
                    current_segment = []
            current_segment.append(line)

        if current_segment:
            segments.append("\n".join(current_segment).strip())

    return segments

# Segmentation du document
segments = segment_text(data)

# Diviser les segments en ensembles d'entraînement et de validation
train_segments, val_segments = train_test_split(segments, test_size=0.2, random_state=42)

# Affichage des segments pour vérification
#for i, segment in enumerate(segments):
#    print(f"Segment {i+1}:\n{segment}\n{'-'*40}\n")

print(len(train_segments))
print(len(val_segments))

# create tokenize function
def tokenize_function(examples):
    # extract text
    text = examples

    # tokenize and truncate text
    tokenizer.truncation_side = "left"
    tokenized_inputs = tokenizer(
        text,
        return_tensors="np",
        truncation=True,
        max_length=512
    )

    # Convertir les array en listes
    tokenized_inputs = {key: value[0].tolist() for key, value in tokenized_inputs.items()}

    return tokenized_inputs

# tokenize training and validation datasets
tokenized_train_data = [tokenize_function(segment) for segment in train_segments]
tokenized_test_data = [tokenize_function(segment) for segment in val_segments]

print(len(tokenized_train_data))
print(tokenized_train_data[1])

# setting pad token
tokenizer.pad_token = tokenizer.eos_token
# data collator
data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)

# Fine-tuning Model

# hyperparameters
lr = 2e-4
batch_size = 4
num_epochs = 10

# define training arguments
training_args = transformers.TrainingArguments(
    output_dir= "Finetuned_Med_GPT",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    logging_strategy="epoch",
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    gradient_accumulation_steps=4,
    warmup_steps=2,
    fp16=True,
    optim="paged_adamw_8bit",

)

# configure trainer
trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_data,
    eval_dataset=tokenized_test_data,
    args=training_args,
    data_collator=data_collator
)


# train model
model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()

# renable warnings
model.config.use_cache = True

