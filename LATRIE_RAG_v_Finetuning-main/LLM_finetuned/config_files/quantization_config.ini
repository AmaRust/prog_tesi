[models]
LLM = ["mistralai/Mistral-7B-Instruct-v0.2"]

[lora]
rank = [8]
lora_alpha = [32]
target_modules = [["q_proj"]]
lora_dropout = [0.05]

[quantization]
used = [0, 1]

[train]
lr = [2e-4]
batch_size = [5]
num_epochs = [500]