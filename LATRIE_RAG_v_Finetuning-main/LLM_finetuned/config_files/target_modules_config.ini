[models]
LLM = ["TheBloke/Mistral-7B-Instruct-v0.2-GPTQ"]

[lora]
rank = [8]
lora_alpha = [64]
target_modules = [["q_proj"], ["q_proj", "v_proj"], ["q_proj", "v_proj", "k_proj"]]
lora_dropout = [0.05]

[quantization]
used = [0]

[train]
lr = [2e-4]
batch_size = [10]
num_epochs = [200]